{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this assignment, we will implement a simple lesk-based WSD. We use SEMCOR WSD dataset for the purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us define a word class containing the following attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text --> the actual word\n",
    "pos --> POS tag of the word\n",
    "lemma --> Lemma of the word\n",
    "wnsn --> wordnet synset id of the sense used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, text, pos=None, lemma=None, wnsn=None, lexsn=None):\n",
    "        self.text = text\n",
    "        self.pos = pos\n",
    "        self.lemma = lemma\n",
    "        self.wnsn = wnsn\n",
    "        self.lexsn = lexsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 84 number of documents \n"
     ]
    }
   ],
   "source": [
    "# let us read a sample xml file\n",
    "tree = ET.parse('semcor/brown2/tagfiles/br-e22.xml')\n",
    "\n",
    "# get the root element\n",
    "root = tree.getroot()\n",
    "\n",
    "documents = []\n",
    "# let us read every sentence, one-by-one\n",
    "# we are ignoring the paragraph structure\n",
    "for sentence_tree in root.findall('context/p/'):\n",
    "    sentence = []\n",
    "#     for every word in that sentence\n",
    "    for word_tree in sentence_tree:\n",
    "#         get the word\n",
    "        word = Word(word_tree.text)\n",
    "        \n",
    "#         if the word xml tag contains info about pos, lemma, wnsn, lexsn, then extract it\n",
    "        if 'pos' in word_tree.attrib:\n",
    "            word.pos = word_tree.attrib['pos']\n",
    "            \n",
    "        if 'lemma' in word_tree.attrib:\n",
    "            word.lemma = word_tree.attrib['lemma']\n",
    "            \n",
    "        if 'wnsn' in word_tree.attrib:\n",
    "            word.wnsn = word_tree.attrib['wnsn']\n",
    "            \n",
    "        if 'lexsn' in word_tree.attrib:\n",
    "            word.lexsn = word_tree.attrib['lexsn']\n",
    "        \n",
    "        sentence.append( word )\n",
    "    documents.append(sentence)\n",
    "\n",
    "print('Read {0} number of documents '.format(len(documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stp = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.6745454545454546\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# window is chosen as 5, vary the window size\n",
    "window = 5\n",
    "\n",
    "# for every sentence\n",
    "for every_sentence in documents:\n",
    "#     let's get the word in the sentence as a list\n",
    "    sentence = [x.text for x in every_sentence] \n",
    "#     print(\"Sentence is {0}\".format(' '.join(sentence))\n",
    "    \n",
    "# for every word in the sentence\n",
    "    for word_index in range(len(every_sentence)):\n",
    "#         not all words have sense info\n",
    "        if every_sentence[word_index].wnsn is not None:\n",
    "            context_bag = []\n",
    "            every_word = every_sentence[word_index]\n",
    "            \n",
    "            for index in range( max(0, word_index - window), min( word_index + window, len(sentence) ) ):\n",
    "                if index == word_index:\n",
    "                    continue\n",
    "                context_bag.append( sentence[index] )\n",
    "                \n",
    "            \n",
    "#             we know the POS tag of the word in the sentence\n",
    "# restrict ourselves to only the senses for that POS category\n",
    "            if every_word.pos.startswith('V'):\n",
    "                synsets = wn.synsets(every_word.text, pos=wn.VERB)\n",
    "            elif every_word.pos.startswith('J'):\n",
    "                synsets = wn.synsets(every_word.text, pos=wn.ADJ)\n",
    "            elif every_word.pos.startswith('R'):\n",
    "                synsets = wn.synsets(every_word.text, pos=wn.ADV)\n",
    "            else:\n",
    "                synsets = wn.synsets(every_word.text, pos=wn.NOUN)\n",
    "                \n",
    "#             all inflections of the word might not be present\n",
    "# search based on lemma\n",
    "            if len(synsets) == 0:\n",
    "                if every_word.pos.startswith('V'):\n",
    "                    synsets = wn.synsets(every_word.lemma, pos=wn.VERB)\n",
    "                elif every_word.pos.startswith('J'):\n",
    "                    synsets = wn.synsets(every_word.lemma, pos=wn.ADJ)\n",
    "                elif every_word.pos.startswith('R'):\n",
    "                    synsets = wn.synsets(every_word.lemma, pos=wn.ADV)\n",
    "                else:\n",
    "                    synsets = wn.synsets(every_word.lemma, pos=wn.NOUN)\n",
    "                \n",
    "            if len(synsets) == 0:\n",
    "                continue\n",
    "            \n",
    "#             find the best synset based on simple word-overlap between word context and synset examples\n",
    "            synset_score = -100\n",
    "            synset_id = \"\"\n",
    "            for every_synset in synsets:\n",
    "                synset_bag = []\n",
    "                for every_synset_example in every_synset.examples():\n",
    "                    synset_bag.extend( every_synset_example.split(' ') )\n",
    "                    \n",
    "                synset_bag.extend(every_synset.definition().split())\n",
    "\n",
    "                sign = []\n",
    "                for each in synset_bag:\n",
    "                    if each.lower() not in stp:\n",
    "                        sign.append(each)\n",
    "                \n",
    " \n",
    "                matching_words = list( set( context_bag ).intersection( set(sign) ) )\n",
    "                if len(matching_words) > synset_score:\n",
    "                    synset_score = len(matching_words)\n",
    "                    synset_id = every_synset.name().split('.')[-1]\n",
    "                    \n",
    "            if synset_id.startswith('0'):\n",
    "                synset_id = synset_id[1:]\n",
    "\n",
    "#             print('Best matching synset id is {0} with overlapping words {1}'.format( synset_id, synset_score ))\n",
    "#             print('Actual synset id is {0}'.format( every_word.wnsn ))\n",
    "            \n",
    "            if synset_id == every_word.wnsn:\n",
    "                correct = correct + 1\n",
    "            total = total + 1\n",
    "\n",
    "print('Accuracy is {0}'.format( (correct * 1.0)/ (total * 1.0) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
