{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this assignment, we will implement a simple lesk-based WSD. We use SEMCOR WSD dataset for the purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us define a word class containing the following attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text --> the actual word\n",
    "pos --> POS tag of the word\n",
    "lemma --> Lemma of the word\n",
    "wnsn --> wordnet synset id of the sense used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, text, pos=None, lemma=None, wnsn=None, lexsn=None):\n",
    "        self.text = text\n",
    "        self.pos = pos\n",
    "        self.lemma = lemma\n",
    "        self.wnsn = wnsn\n",
    "        self.lexsn = lexsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['br-e22.xml', 'br-e23.xml', 'br-e25.xml', 'br-e26.xml', 'br-e27.xml', 'br-e28.xml', 'br-e30.xml', 'br-e31.xml', 'br-f08.xml', 'br-f13.xml', 'br-f14.xml', 'br-f15.xml', 'br-f16.xml', 'br-f17.xml', 'br-f18.xml', 'br-f20.xml', 'br-f21.xml', 'br-f22.xml', 'br-f23.xml', 'br-f24.xml', 'br-f25.xml', 'br-f33.xml', 'br-f44.xml', 'br-g12.xml', 'br-g14.xml', 'br-g16.xml', 'br-g17.xml', 'br-g18.xml', 'br-g19.xml', 'br-g20.xml', 'br-g21.xml', 'br-g22.xml', 'br-g23.xml', 'br-g28.xml', 'br-g31.xml', 'br-g39.xml', 'br-g43.xml', 'br-g44.xml', 'br-h09.xml', 'br-h11.xml', 'br-h12.xml', 'br-h13.xml', 'br-h14.xml', 'br-h15.xml', 'br-h16.xml', 'br-h17.xml', 'br-h18.xml', 'br-h21.xml', 'br-h24.xml', 'br-j29.xml', 'br-j30.xml', 'br-j31.xml', 'br-j32.xml', 'br-j33.xml', 'br-j34.xml', 'br-j35.xml', 'br-j38.xml', 'br-j41.xml', 'br-j42.xml', 'br-l08.xml', 'br-l09.xml', 'br-l10.xml', 'br-l13.xml', 'br-l14.xml', 'br-l15.xml', 'br-l16.xml', 'br-l17.xml', 'br-l18.xml', 'br-n09.xml', 'br-n10.xml', 'br-n11.xml', 'br-n12.xml', 'br-n14.xml', 'br-n15.xml', 'br-n16.xml', 'br-n17.xml', 'br-n20.xml', 'br-p07.xml', 'br-p09.xml', 'br-p10.xml', 'br-p12.xml', 'br-p24.xml', 'br-r04.xml']\n"
     ]
    }
   ],
   "source": [
    "files_ls = []\n",
    "for file in os.listdir(\"semcor/brown2/tagfiles/\"):\n",
    "    if file.endswith(\".xml\"):\n",
    "        files_ls.append(file)\n",
    "print(files_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "# let us read a sample xml file\n",
    "doc_ls = []\n",
    "for each_file in files_ls:\n",
    "    tree = ET.parse('semcor/brown2/tagfiles/' + each_file)\n",
    "\n",
    "    # get the root element\n",
    "    root = tree.getroot()\n",
    "    # print(root)\n",
    "    documents = []\n",
    "    # let us read every sentence, one-by-one\n",
    "    # we are ignoring the paragraph structure\n",
    "    for sentence_tree in root.findall('context/p/'):\n",
    "        sentence = []\n",
    "    #     for every word in that sentence\n",
    "        for word_tree in sentence_tree:\n",
    "    #         get the word\n",
    "            word = Word(word_tree.text)\n",
    "        \n",
    "    #         if the word xml tag contains info about pos, lemma, wnsn, lexsn, then extract it\n",
    "            if 'pos' in word_tree.attrib:\n",
    "                word.pos = word_tree.attrib['pos']\n",
    "            \n",
    "            if 'lemma' in word_tree.attrib:\n",
    "                word.lemma = word_tree.attrib['lemma']\n",
    "            \n",
    "            if 'wnsn' in word_tree.attrib:\n",
    "                word.wnsn = word_tree.attrib['wnsn']\n",
    "            \n",
    "            if 'lexsn' in word_tree.attrib:\n",
    "                word.lexsn = word_tree.attrib['lexsn']\n",
    "        \n",
    "            sentence.append( word )\n",
    "        documents.append(sentence)\n",
    "\n",
    "#     print(documents)\n",
    "    doc_ls.append(documents)\n",
    "#     print('Read {0} number of documents '.format(len(documents)))\n",
    "\n",
    "print(len(doc_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "[0.6727272727272727, 0.6333973128598849, 0.6866359447004609, 0.6551040634291377, 0.7806267806267806, 0.6453110492107706, 0.6838235294117647, 0.6772438803263826, 0.6817769718948323, 0.704052780395853, 0.6356192425793245, 0.6137667304015296, 0.7152255639097744, 0.7180020811654526, 0.6975124378109453, 0.6890424481737414, 0.681075888568684, 0.6620959843290891, 0.6579961464354528, 0.6802656546489564, 0.7303877366997295, 0.6908023483365949, 0.6809480401093893, 0.6167176350662589, 0.6598639455782312, 0.5954198473282443, 0.643574297188755, 0.584349593495935, 0.6739345887016849, 0.6705539358600583, 0.6132478632478633, 0.6487985212569316, 0.6881188118811881, 0.6855983772819473, 0.6835187057633973, 0.6633064516129032, 0.6035502958579881, 0.6413793103448275, 0.7046580773042617, 0.7455565949485501, 0.6991341991341992, 0.6445725264169068, 0.712776176753122, 0.704, 0.6977911646586346, 0.6995884773662552, 0.6147368421052631, 0.6889726672950047, 0.7515856236786469, 0.707227813357731, 0.6237524950099801, 0.7197518097207859, 0.7217217217217218, 0.6780442804428044, 0.680073126142596, 0.5956678700361011, 0.7055267702936097, 0.685972850678733, 0.6747437092264679, 0.6579476861167002, 0.7131630648330058, 0.6782700421940928, 0.662778366914104, 0.6597014925373135, 0.685336048879837, 0.6510468594217348, 0.6825557809330629, 0.6600441501103753, 0.694206008583691, 0.7118126272912424, 0.6688867745004757, 0.6901544401544402, 0.623725671918443, 0.6640625, 0.7060439560439561, 0.6784188034188035, 0.6643495531281033, 0.6633858267716536, 0.6383981154299175, 0.6418219461697723, 0.6608315098468271, 0.7184661957618567, 0.6806629834254143]\n"
     ]
    }
   ],
   "source": [
    "# To calculate accuracy\n",
    "stop_words = set(stopwords.words('english'))\n",
    "acc_ls = []\n",
    "for every_doc in doc_ls:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # window size varies the context window\n",
    "    window = 3\n",
    "\n",
    "    # for every sentence\n",
    "    for every_sentence in every_doc:\n",
    "        # let's get the word in the sentence as a list\n",
    "        sentence = [x.text for x in every_sentence] \n",
    "    #     print(\"Sentence is {0}\".format(' '.join(sentence)))\n",
    "    \n",
    "        # removing punctuations from sentence list\n",
    "        sentence = [''.join(chr for chr in s if chr not in string.punctuation) for s in sentence] \n",
    "        sentence = [s for s in sentence if s]\n",
    "        # Removing stop words from sentence for getting better context window\n",
    "        sentence = [w for w in sentence if not w.lower() in stop_words]\n",
    "        \n",
    "        # for every word in the sentence\n",
    "        for word_index in range(len(every_sentence)):\n",
    "            # not all words have sense info\n",
    "            if every_sentence[word_index].wnsn is not None:\n",
    "                context_bag = []\n",
    "                every_word = every_sentence[word_index]\n",
    "            \n",
    "                for index in range( max(0, word_index - window), min( word_index + window, len(sentence) ) ):\n",
    "                    if index == word_index:\n",
    "                        continue\n",
    "                    context_bag.append( sentence[index] )\n",
    "                \n",
    "#                 print('Context bag', context_bag)\n",
    "            \n",
    "                # we know the POS tag of the word in the sentence restrict ourselves to only the senses for that POS category\n",
    "                if every_word.pos.startswith('V'):\n",
    "                    synsets = wn.synsets(every_word.text, pos=wn.VERB)\n",
    "                elif every_word.pos.startswith('J'):\n",
    "                    synsets = wn.synsets(every_word.text, pos=wn.ADJ)\n",
    "                elif every_word.pos.startswith('R'):\n",
    "                    synsets = wn.synsets(every_word.text, pos=wn.ADV)\n",
    "                else:\n",
    "                    synsets = wn.synsets(every_word.text, pos=wn.NOUN)\n",
    "                \n",
    "                # all inflections of the word might not be present search based on lemma\n",
    "                if len(synsets) == 0:\n",
    "                    if every_word.pos.startswith('V'):\n",
    "                        synsets = wn.synsets(every_word.lemma, pos=wn.VERB)\n",
    "                    elif every_word.pos.startswith('J'):\n",
    "                        synsets = wn.synsets(every_word.lemma, pos=wn.ADJ)\n",
    "                    elif every_word.pos.startswith('R'):\n",
    "                        synsets = wn.synsets(every_word.lemma, pos=wn.ADV)\n",
    "                    else:\n",
    "                        synsets = wn.synsets(every_word.lemma, pos=wn.NOUN)\n",
    "                \n",
    "                if len(synsets) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # find the best synset based on simple word-overlap between word context and synset examples\n",
    "                synset_score = -100\n",
    "                synset_id = \"\"\n",
    "                ############################ Adding words eg and gloss ##################################\n",
    "                for every_synset in synsets:\n",
    "                    synset_bag = []\n",
    "                    for every_synset_example in every_synset.examples():\n",
    "                        synset_bag.extend( every_synset_example.split(' ') )\n",
    "                \n",
    "                    gloss_ls = []\n",
    "                    defi = every_synset.definition().split()\n",
    "                    for each_wrd in defi: \n",
    "                        def_no_punc = re.sub(r'[^\\w\\s]', '', each_wrd) # removing punctuation from each word in list\n",
    "                        gloss_ls.append(def_no_punc)\n",
    "                    \n",
    "                    synset_bag.extend(gloss_ls)\n",
    "                \n",
    "                    ############################ Adding hypernym eg and gloss ##################################\n",
    "                    hypern = every_synset.hypernyms()\n",
    "                    for each_hyper in hypern:\n",
    "                        for hyper_eg in each_hyper.examples():\n",
    "                            synset_bag.extend(hyper_eg.split()) # Adding hypernyms example to sign \n",
    "                        hyper_gloss = []\n",
    "                        hyper_def = each_hyper.definition().split()\n",
    "                        for e in hyper_def:\n",
    "                            hyper_gloss.append(re.sub(r'[^\\w\\s]', '', e))\n",
    "                        synset_bag.extend(hyper_gloss) # Adding hypernyms gloss to sign\n",
    "                \n",
    "                    ############################ Adding hyponym eg and gloss ##################################\n",
    "                    hypon = every_synset.hyponyms()\n",
    "                    for each_hypo in hypon:\n",
    "                        for hypo_eg in each_hypo.examples():\n",
    "                            synset_bag.extend(hypo_eg.split()) # Adding hyponyms example to sign \n",
    "                        hypo_gloss = []\n",
    "                        hypo_def = each_hypo.definition().split()\n",
    "                        for e in hypo_def:\n",
    "                            hypo_gloss.append(re.sub(r'[^\\w\\s]', '', e))\n",
    "                        synset_bag.extend(hypo_gloss) # Adding hyponyms gloss to sign\n",
    "                \n",
    "#                     print('Synset bag', synset_bag)\n",
    "                    ############################ Filtering signature for stopwords ##################################\n",
    "                    filtered_synset_bag = [w for w in synset_bag if not w.lower() in stop_words] # stop word removal\n",
    "                                \n",
    "                    ############################ Checking overlap between context and extended signature ############################    \n",
    "                    matching_words = list( set( context_bag ).intersection( set(filtered_synset_bag) ) )\n",
    "                    if len(matching_words) > synset_score:\n",
    "                        synset_score = len(matching_words)\n",
    "                        synset_id = every_synset.name().split('.')[-1]\n",
    "                    \n",
    "                if synset_id.startswith('0'):\n",
    "                    synset_id = synset_id[1:]\n",
    "\n",
    "    #             print('Best matching synset id is {0} with overlapping words {1}'.format( synset_id, synset_score ))\n",
    "    #             print('Actual synset id is {0}'.format( every_word.wnsn ))\n",
    "            \n",
    "                if synset_id == every_word.wnsn:\n",
    "                    correct = correct + 1\n",
    "                total = total + 1\n",
    "\n",
    "#     print('Accuracy is {0}'.format( (correct * 1.0)/ (total * 1.0) ))\n",
    "    acc_ls.append((correct * 1.0)/ (total * 1.0))\n",
    "    \n",
    "print(len(acc_ls))\n",
    "print(acc_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum accuracy 0.7806267806267806 is obtained in file br-e27.xml\n"
     ]
    }
   ],
   "source": [
    "file_num = 0\n",
    "max_acc = 0\n",
    "for i in range(len(acc_ls)):\n",
    "    if acc_ls[i] > max_acc:\n",
    "        max_acc = acc_ls[i]\n",
    "        file_num = i\n",
    "        \n",
    "print('Maximum accuracy {0} is obtained in file {1}'.format(max_acc, files_ls[file_num]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some notable observations from the expreiment\n",
    "- Initially in the original code accuracy incresed in the range 55.36 to 65.63 for context window in range 7 to 1 respectively. \n",
    "- Just by adding gloss(definition) to the signature, accuracy showed a dip by an average of 1 for corresponding window sizes.\n",
    "- By removing stop words from the context and signature(gloss + example), a significant increase in accuracy was observed across different window sizes. (since it considered more important words)\n",
    "- Apart from stopwords punctuations were also removed from the lists. \n",
    "- By further adding hypernyms and hyponyms gloss and examples to signature for the words, the accuracy scores showed even better accuracy. (more chances for getting overlap)\n",
    "- Window size was an important parameter in this experiment, with a window size of around 2-4 across files better accuracies were obtained as compared to larger window sizes(>5 or 6). (This maybe because in most sentences, the words around the context word are more significant in shorter span size, increasing this window do not give any significant improvements, and even dropped few fractional accuracy points maybe because it included more irrelevant words in the context which may have increased overlap with other irrelevant sense)(Also the greater window size may have better impact in those datasets where there are long dependencies or relevance among words, like some compound sentences or phrases).\n",
    "- Another approach which was not tried here, but maybe tried later was instead of removing stopwords using TF-IDF weighing for frequent words as mentioned in lectures also."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
